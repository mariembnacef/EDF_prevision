import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import datetime as dt
import joblib
from dateutil.easter import easter
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor

def preparer_donnees(chemin_fichier):
    """
    Fonction pour charger et pr√©parer les donn√©es temporelles
    """
    # Chargement des donn√©es
    df = pd.read_csv(chemin_fichier, sep="\t", encoding="latin1")
    
    # Conversion des colonnes Date et Heures
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df['Heures'] = pd.to_datetime(df['Heures'], format='%H:%M', errors='coerce').dt.time
    
    # Variables temporelles
    df['mois'] = df['Date'].dt.month
    df['annee'] = df['Date'].dt.year
    df['jour_semaine'] = df['Date'].dt.weekday + 1
    
    # Encodage des variables cat√©gorielles
    saison_mapping = {'Automne': 1, 'Hiver': 2, 'Printemps': 3, '√ât√©': 4}
    df['saison_num'] = df['Saison'].map(saison_mapping)
    
    tempo_mapping = {'BLEU': 1, 'BLANC': 2, 'ROUGE': 3}
    df['tempo_num'] = df['Type de jour TEMPO'].map(tempo_mapping)
    
    # Encodage p√©riode de la journ√©e
    def encoder_periode_jour(time_obj):
        if pd.isnull(time_obj): return None
        if dt.time(0, 0) <= time_obj < dt.time(5, 0): return 1  # nuit
        elif dt.time(5, 0) <= time_obj < dt.time(12, 0): return 2  # matin
        elif dt.time(12, 0) <= time_obj < dt.time(18, 0): return 3  # apr√®s-midi
        else: return 4  # soir
    
    df['periode_jour_code'] = df['Heures'].apply(encoder_periode_jour)
    
    # Jours f√©ri√©s
    def get_french_holidays(year):
        fixed = [
            dt.date(year, 1, 1), dt.date(year, 5, 1), dt.date(year, 5, 8),
            dt.date(year, 7, 14), dt.date(year, 8, 15), dt.date(year, 11, 1),
            dt.date(year, 11, 11), dt.date(year, 12, 25)
        ]
        easter_date = easter(year)
        movable = [
            easter_date + dt.timedelta(days=1),   # Lundi de P√¢ques
            easter_date + dt.timedelta(days=39),  # Ascension
            easter_date + dt.timedelta(days=50),  # Lundi de Pentec√¥te
        ]
        return fixed + movable
    
    years = range(df['Date'].dt.year.min(), df['Date'].dt.year.max() + 1)
    all_holidays = [date for year in years for date in get_french_holidays(year)]
    all_holidays = pd.to_datetime(all_holidays)
    
    df['jour_ferie'] = df['Date'].dt.normalize().isin(all_holidays)
    
    # Conversion des heures en format num√©rique
    df['Heures'] = pd.to_datetime(df['Heures'], format='%H:%M:%S', errors='coerce')
    df['Heures_float'] = df['Heures'].dt.hour + df['Heures'].dt.minute / 60
    
    # Tri chronologique et cr√©ation de lags
    df = df.sort_values(['Date', 'Heures_float']).reset_index(drop=True)
    for lag in [1, 2, 3, 4]:
        df[f'lag_{lag}'] = df['Consommation'].shift(lag)
    
    df = df.dropna().reset_index(drop=True)
    
    # Supprimer les colonnes inutiles pour le mod√®le
    df_model = df.drop(columns=['Type de jour TEMPO', 'Date', 'Heures', 'Pr√©vision J', 
                               'Pr√©vision J-1', 'Jour', 'Saison'])
    df_model = df_model.dropna()
    
    # S√©paration features / target
    X = df_model.drop(columns=['Consommation'])
    y = df_model['Consommation']
    
    return X, y, df_model

def entrainer_modele(X, y, verbose=True):
    """
    Fonction pour entra√Æner et optimiser le mod√®le XGBoost
    """
    # Split temporel (pas de shuffle car s√©rie temporelle)
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    # Grille large pour exploration RandomizedSearchCV
    param_dist = {
        'n_estimators': [100, 300, 500, 700, 1000],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 5, 6, 8],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1, 1.5],
    }
    
    # Mod√®le de base
    xgb_base = XGBRegressor(objective='reg:squarederror', random_state=42)
    
    # Recherche al√©atoire des hyperparam√®tres
    random_search = RandomizedSearchCV(
        estimator=xgb_base,
        param_distributions=param_dist,
        n_iter=30,
        scoring='neg_root_mean_squared_error',
        cv=3,
        verbose=1 if verbose else 0,
        n_jobs=-1
    )
    
    random_search.fit(X_train, y_train)
    
    if verbose:
        print("üîç Meilleurs param√®tres RandomizedSearchCV :")
        print(random_search.best_params_)
    
    # R√©cup√©rer les meilleurs param√®tres comme base
    best_params = random_search.best_params_
    
    # Grille affin√©e autour des meilleures valeurs
    param_grid = {
        'n_estimators': [best_params['n_estimators'] - 100, best_params['n_estimators'], best_params['n_estimators'] + 100],
        'learning_rate': [best_params['learning_rate'] / 2, best_params['learning_rate'], best_params['learning_rate'] * 2],
        'max_depth': [best_params['max_depth'] - 1, best_params['max_depth'], best_params['max_depth'] + 1],
        'subsample': [best_params['subsample']],
        'colsample_bytree': [best_params['colsample_bytree']],
        'reg_alpha': [best_params['reg_alpha']],
        'reg_lambda': [best_params['reg_lambda']]
    }
    
    xgb_tuned = XGBRegressor(objective='reg:squarederror', random_state=42)
    
    # Recherche sur grille cibl√©e
    grid_search = GridSearchCV(
        estimator=xgb_tuned,
        param_grid=param_grid,
        scoring='neg_root_mean_squared_error',
        cv=3,
        verbose=1 if verbose else 0,
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    if verbose:
        print("üèÜ Meilleurs param√®tres apr√®s GridSearch cibl√© :")
        print(grid_search.best_params_)
    
    # R√©cup√©rer le meilleur mod√®le
    best_model = grid_search.best_estimator_
    
    return best_model, X_train, X_val, y_train, y_val

def evaluer_modele(model, X_train, X_val, y_train, y_val):
    """
    Fonction pour √©valuer les performances du mod√®le et d√©tecter l'overfitting
    """
    # Pr√©dictions
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    
    # M√©triques d'√©valuation
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    r2_train = r2_score(y_train, y_train_pred)
    
    rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))
    r2_val = r2_score(y_val, y_val_pred)
    
    # V√©rification de l'overfitting
    diff_r2 = r2_train - r2_val
    status = "Overfitting d√©tect√©" if diff_r2 > 0.05 else "Pas (ou peu) d'overfitting"
    
    # Affichage des r√©sultats
    print(f"Train R¬≤: {r2_train:.4f}, Train RMSE: {rmse_train:.2f}")
    print(f"Test  R¬≤: {r2_val:.4f}, Test  RMSE: {rmse_val:.2f}")
    print(f"Diff√©rence R¬≤ (Train - Test): {diff_r2:.4f} ‚Üí {status}")
    print(f"üìâ RMSE finale : {rmse_val:.2f}")
    print(f"üìà R¬≤ (score de d√©termination) : {r2_val:.4f}")
    
    return {
        'rmse_train': rmse_train,
        'r2_train': r2_train,
        'rmse_val': rmse_val,
        'r2_val': r2_val,
        'overfitting': diff_r2 > 0.05
    }

def sauvegarder_modele(model, nom_fichier="models/xgboost_conso_best_model_hybride.pkl"):
    """
    Fonction pour sauvegarder le mod√®le entrain√©
    """
    # Cr√©er le r√©pertoire si n√©cessaire
    os.makedirs(os.path.dirname(nom_fichier), exist_ok=True)
    
    # Sauvegarder le mod√®le
    joblib.dump(model, nom_fichier)
    print(f"‚úÖ Mod√®le sauvegard√© sous '{nom_fichier}'")
    
    return nom_fichier

def visualiser_resultats(y_test, y_pred, features_importance=None):
    """
    Fonction pour visualiser les r√©sultats du mod√®le
    """
    # Cr√©ation de la figure
    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))
    
    # Premier graphique: pr√©dictions vs r√©alit√©
    axes[0].scatter(y_test, y_pred, alpha=0.5)
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    axes[0].set_xlabel('Valeurs r√©elles')
    axes[0].set_ylabel('Pr√©dictions')
    axes[0].set_title('Comparaison des pr√©dictions avec les valeurs r√©elles')
    
    # Second graphique: distribution des erreurs
    erreurs = y_test - y_pred
    sns.histplot(erreurs, kde=True, ax=axes[1])
    axes[1].set_xlabel('Erreur de pr√©diction')
    axes[1].set_ylabel('Fr√©quence')
    axes[1].set_title('Distribution des erreurs de pr√©diction')
    
    plt.tight_layout()
    plt.show()
    
    # Affichage de l'importance des variables si disponible
    if features_importance is not None:
        plt.figure(figsize=(12, 6))
        features_importance.plot(kind='barh')
        plt.title('Importance des variables')
        plt.tight_layout()
        plt.show()

def main():
    """
    Fonction principale qui orchestre tout le processus
    """
    # 1. Pr√©paration des donn√©es
    print("1. Pr√©paration des donn√©es...")
    X, y, df_model = preparer_donnees("..../data/df_filtre.csv")
    print(f"‚úÖ Donn√©es pr√©par√©es: {X.shape[0]} √©chantillons avec {X.shape[1]} caract√©ristiques")
    
    # 2. Entra√Ænement du mod√®le
    print("\n2. Entra√Ænement du mod√®le et optimisation des hyperparam√®tres...")
    best_model, X_train, X_val, y_train, y_val = entrainer_modele(X, y)
    
    # 3. √âvaluation du mod√®le
    print("\n3. √âvaluation des performances du mod√®le...")
    resultats = evaluer_modele(best_model, X_train, X_val, y_train, y_val)
    
    # 4. Sauvegarde du mod√®le
    print("\n4. Sauvegarde du mod√®le...")
    chemin_modele = sauvegarder_modele(best_model)
    
    # 5. Visualisation des r√©sultats (optionnel)
    # visualiser_resultats(y_val, best_model.predict(X_val))
    
    print("\nProcessus termin√© avec succ√®s!")
    return best_model, resultats

if __name__ == "__main__":
    main()